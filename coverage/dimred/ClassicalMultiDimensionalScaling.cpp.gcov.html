<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en">

<head>
<meta name="robots" content="noindex">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LCOV - plumed test coverage - dimred/ClassicalMultiDimensionalScaling.cpp</title>
  <link rel="stylesheet" type="text/css" href="../gcov.css">
</head>

<body>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="title">LCOV - code coverage report</td></tr>
    <tr><td class="ruler"><img src="../glass.png" width=3 height=3 alt=""></td></tr>

    <tr>
      <td width="100%">
        <table cellpadding=1 border=0 width="100%">
          <tr>
            <td width="10%" class="headerItem">Current view:</td>
            <td width="35%" class="headerValue"><a href="../index.html">top level</a> - <a href="index.html">dimred</a> - ClassicalMultiDimensionalScaling.cpp<span style="font-size: 80%;"> (source / <a href="ClassicalMultiDimensionalScaling.cpp.func-sort-c.html">functions</a>)</span></td>
            <td width="5%"></td>
            <td width="15%"></td>
            <td width="10%" class="headerCovTableHead">Hit</td>
            <td width="10%" class="headerCovTableHead">Total</td>
            <td width="15%" class="headerCovTableHead">Coverage</td>
          </tr>
          <tr>
            <td class="headerItem">Test:</td>
            <td class="headerValue">plumed test coverage</td>
            <td></td>
            <td class="headerItem">Lines:</td>
            <td class="headerCovTableEntry">23</td>
            <td class="headerCovTableEntry">23</td>
            <td class="headerCovTableEntryHi">100.0 %</td>
          </tr>
          <tr>
            <td class="headerItem">Date:</td>
            <td class="headerValue">2019-08-13 10:15:31</td>
            <td></td>
            <td class="headerItem">Functions:</td>
            <td class="headerCovTableEntry">10</td>
            <td class="headerCovTableEntry">11</td>
            <td class="headerCovTableEntryHi">90.9 %</td>
          </tr>
          <tr><td><img src="../glass.png" width=3 height=3 alt=""></td></tr>
        </table>
      </td>
    </tr>

    <tr><td class="ruler"><img src="../glass.png" width=3 height=3 alt=""></td></tr>
  </table>

  <table cellpadding=0 cellspacing=0 border=0>
    <tr>
      <td><br></td>
    </tr>
    <tr>
      <td>
<pre class="sourceHeading">          Line data    Source code</pre>
<pre class="source">
<a name="1"><span class="lineNum">       1 </span>            : /* +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</a>
<a name="2"><span class="lineNum">       2 </span>            :    Copyright (c) 2015-2019 The plumed team</a>
<a name="3"><span class="lineNum">       3 </span>            :    (see the PEOPLE file at the root of the distribution for a list of names)</a>
<a name="4"><span class="lineNum">       4 </span>            : </a>
<a name="5"><span class="lineNum">       5 </span>            :    See http://www.plumed.org for more information.</a>
<a name="6"><span class="lineNum">       6 </span>            : </a>
<a name="7"><span class="lineNum">       7 </span>            :    This file is part of plumed, version 2.</a>
<a name="8"><span class="lineNum">       8 </span>            : </a>
<a name="9"><span class="lineNum">       9 </span>            :    plumed is free software: you can redistribute it and/or modify</a>
<a name="10"><span class="lineNum">      10 </span>            :    it under the terms of the GNU Lesser General Public License as published by</a>
<a name="11"><span class="lineNum">      11 </span>            :    the Free Software Foundation, either version 3 of the License, or</a>
<a name="12"><span class="lineNum">      12 </span>            :    (at your option) any later version.</a>
<a name="13"><span class="lineNum">      13 </span>            : </a>
<a name="14"><span class="lineNum">      14 </span>            :    plumed is distributed in the hope that it will be useful,</a>
<a name="15"><span class="lineNum">      15 </span>            :    but WITHOUT ANY WARRANTY; without even the implied warranty of</a>
<a name="16"><span class="lineNum">      16 </span>            :    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</a>
<a name="17"><span class="lineNum">      17 </span>            :    GNU Lesser General Public License for more details.</a>
<a name="18"><span class="lineNum">      18 </span>            : </a>
<a name="19"><span class="lineNum">      19 </span>            :    You should have received a copy of the GNU Lesser General Public License</a>
<a name="20"><span class="lineNum">      20 </span>            :    along with plumed.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</a>
<a name="21"><span class="lineNum">      21 </span>            : +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ */</a>
<a name="22"><span class="lineNum">      22 </span>            : #include &quot;DimensionalityReductionBase.h&quot;</a>
<a name="23"><span class="lineNum">      23 </span>            : #include &quot;core/ActionRegister.h&quot;</a>
<a name="24"><span class="lineNum">      24 </span>            : </a>
<a name="25"><span class="lineNum">      25 </span>            : //+PLUMEDOC DIMRED CLASSICAL_MDS</a>
<a name="26"><span class="lineNum">      26 </span>            : /*</a>
<a name="27"><span class="lineNum">      27 </span>            : Create a low-dimensional projection of a trajectory using the classical multidimensional</a>
<a name="28"><span class="lineNum">      28 </span>            :  scaling algorithm.</a>
<a name="29"><span class="lineNum">      29 </span>            : </a>
<a name="30"><span class="lineNum">      30 </span>            : Multidimensional scaling (MDS) is similar to what is done when you make a map. You start with distances</a>
<a name="31"><span class="lineNum">      31 </span>            : between London, Belfast, Paris and Dublin and then you try to arrange points on a piece of paper so that the (suitably scaled)</a>
<a name="32"><span class="lineNum">      32 </span>            : distances between the points in your map representing each of those cities are related to the true distances between the cities.</a>
<a name="33"><span class="lineNum">      33 </span>            : Stating this more mathematically MDS endeavors to find an &lt;a href=&quot;http://en.wikipedia.org/wiki/Isometry&quot;&gt;isometry&lt;/a&gt;</a>
<a name="34"><span class="lineNum">      34 </span>            : between points distributed in a high-dimensional space and a set of points distributed in a low-dimensional plane.</a>
<a name="35"><span class="lineNum">      35 </span>            : In other words, if we have \f$M\f$ \f$D\f$-dimensional points, \f$\mathbf{X}\f$,</a>
<a name="36"><span class="lineNum">      36 </span>            : and we can calculate dissimilarities between pairs them, \f$D_{ij}\f$, we can, with an MDS calculation, try to create \f$M\f$ projections,</a>
<a name="37"><span class="lineNum">      37 </span>            : \f$\mathbf{x}\f$, of the high dimensionality points in a \f$d\f$-dimensional linear space by trying to arrange the projections so that the</a>
<a name="38"><span class="lineNum">      38 </span>            : Euclidean distances between pairs of them, \f$d_{ij}\f$, resemble the dissimilarities between the high dimensional points.  In short we minimize:</a>
<a name="39"><span class="lineNum">      39 </span>            : </a>
<a name="40"><span class="lineNum">      40 </span>            : \f[</a>
<a name="41"><span class="lineNum">      41 </span>            : \chi^2 = \sum_{i \ne j} \left( D_{ij} - d_{ij} \right)^2</a>
<a name="42"><span class="lineNum">      42 </span>            : \f]</a>
<a name="43"><span class="lineNum">      43 </span>            : </a>
<a name="44"><span class="lineNum">      44 </span>            : where \f$D_{ij}\f$ is the distance between point \f$X^{i}\f$ and point \f$X^{j}\f$ and \f$d_{ij}\f$ is the distance between the projection</a>
<a name="45"><span class="lineNum">      45 </span>            : of \f$X^{i}\f$, \f$x^i\f$, and the projection of \f$X^{j}\f$, \f$x^j\f$.  A tutorial on this approach can be used to analyze simulations</a>
<a name="46"><span class="lineNum">      46 </span>            : can be found in the tutorial \ref belfast-3 and in the following &lt;a href=&quot;https://www.youtube.com/watch?v=ofC2qz0_9_A&amp;feature=youtu.be&quot; &gt; short video.&lt;/a&gt;</a>
<a name="47"><span class="lineNum">      47 </span>            : </a>
<a name="48"><span class="lineNum">      48 </span>            : \par Examples</a>
<a name="49"><span class="lineNum">      49 </span>            : </a>
<a name="50"><span class="lineNum">      50 </span>            : The following command instructs plumed to construct a classical multidimensional scaling projection of a trajectory.</a>
<a name="51"><span class="lineNum">      51 </span>            : The RMSD distance between atoms 1-256 have moved is used to measure the distances in the high-dimensional space.</a>
<a name="52"><span class="lineNum">      52 </span>            : </a>
<a name="53"><span class="lineNum">      53 </span>            : \plumedfile</a>
<a name="54"><span class="lineNum">      54 </span>            : data: COLLECT_FRAMES ATOMS=1-256</a>
<a name="55"><span class="lineNum">      55 </span>            : mat: EUCLIDEAN_DISSIMILARITIES USE_OUTPUT_DATA_FROM=data</a>
<a name="56"><span class="lineNum">      56 </span>            : mds: CLASSICAL_MDS USE_OUTPUT_DATA_FROM=mat NLOW_DIM=2</a>
<a name="57"><span class="lineNum">      57 </span>            : OUTPUT_ANALYSIS_DATA_TO_COLVAR USE_OUTPUT_DATA_FROM=mds FILE=rmsd-embed</a>
<a name="58"><span class="lineNum">      58 </span>            : \endplumedfile</a>
<a name="59"><span class="lineNum">      59 </span>            : </a>
<a name="60"><span class="lineNum">      60 </span>            : The following section is for people who are interested in how this method works in detail. A solid understanding of this material is</a>
<a name="61"><span class="lineNum">      61 </span>            : not necessary to use MDS.</a>
<a name="62"><span class="lineNum">      62 </span>            : </a>
<a name="63"><span class="lineNum">      63 </span>            : \section dim-sec Method of optimization</a>
<a name="64"><span class="lineNum">      64 </span>            : </a>
<a name="65"><span class="lineNum">      65 </span>            : The stress function can be minimized using a standard optimization algorithm such as conjugate gradients or steepest descent.</a>
<a name="66"><span class="lineNum">      66 </span>            : However, it is more common to do this minimization using a technique known as classical scaling.  Classical scaling works by</a>
<a name="67"><span class="lineNum">      67 </span>            : recognizing that each of the distances $D_{ij}$ in the above sum can be written as:</a>
<a name="68"><span class="lineNum">      68 </span>            : </a>
<a name="69"><span class="lineNum">      69 </span>            : \f[</a>
<a name="70"><span class="lineNum">      70 </span>            : D_{ij}^2 = \sum_{\alpha} (X^i_\alpha - X^j_\alpha)^2 = \sum_\alpha (X^i_\alpha)^2 + (X^j_\alpha)^2 - 2X^i_\alpha X^j_\alpha</a>
<a name="71"><span class="lineNum">      71 </span>            : \f]</a>
<a name="72"><span class="lineNum">      72 </span>            : </a>
<a name="73"><span class="lineNum">      73 </span>            : We can use this expression and matrix algebra to calculate multiple distances at once.  For instance if we have three points,</a>
<a name="74"><span class="lineNum">      74 </span>            : \f$\mathbf{X}\f$, we can write distances between them as:</a>
<a name="75"><span class="lineNum">      75 </span>            : </a>
<a name="76"><span class="lineNum">      76 </span>            : \f{eqnarray*}{</a>
<a name="77"><span class="lineNum">      77 </span>            : D^2(\mathbf{X}) &amp;=&amp; \left[ \begin{array}{ccc}</a>
<a name="78"><span class="lineNum">      78 </span>            : 0 &amp; d_{12}^2 &amp; d_{13}^2 \\</a>
<a name="79"><span class="lineNum">      79 </span>            : d_{12}^2 &amp; 0 &amp; d_{23}^2 \\</a>
<a name="80"><span class="lineNum">      80 </span>            : d_{13}^2 &amp; d_{23}^2 &amp; 0</a>
<a name="81"><span class="lineNum">      81 </span>            : \end{array}\right] \\</a>
<a name="82"><span class="lineNum">      82 </span>            : &amp;=&amp;</a>
<a name="83"><span class="lineNum">      83 </span>            : \sum_\alpha \left[ \begin{array}{ccc}</a>
<a name="84"><span class="lineNum">      84 </span>            : (X^1_\alpha)^2 &amp; (X^1_\alpha)^2 &amp; (X^1_\alpha)^2 \\</a>
<a name="85"><span class="lineNum">      85 </span>            : (X^2_\alpha)^2 &amp; (X^2_\alpha)^2 &amp; (X^2_\alpha)^2 \\</a>
<a name="86"><span class="lineNum">      86 </span>            : (X^3_\alpha)^2 &amp; (X^3_\alpha)^2 &amp; (X^3_\alpha)^2 \\</a>
<a name="87"><span class="lineNum">      87 </span>            : \end{array}\right]</a>
<a name="88"><span class="lineNum">      88 </span>            :  + \sum_\alpha \left[ \begin{array}{ccc}</a>
<a name="89"><span class="lineNum">      89 </span>            : (X^1_\alpha)^2 &amp; (X^2_\alpha)^2 &amp; (X^3_\alpha)^2 \\</a>
<a name="90"><span class="lineNum">      90 </span>            : (X^1_\alpha)^2 &amp; (X^2_\alpha)^2 &amp; (X^3_\alpha)^2 \\</a>
<a name="91"><span class="lineNum">      91 </span>            : (X^1_\alpha)^2 &amp; (X^2_\alpha)^2 &amp; (X^3_\alpha)^2 \\</a>
<a name="92"><span class="lineNum">      92 </span>            : \end{array}\right]</a>
<a name="93"><span class="lineNum">      93 </span>            : - 2 \sum_\alpha \left[ \begin{array}{ccc}</a>
<a name="94"><span class="lineNum">      94 </span>            : X^1_\alpha X^1_\alpha &amp; X^1_\alpha X^2_\alpha &amp; X^1_\alpha X^3_\alpha \\</a>
<a name="95"><span class="lineNum">      95 </span>            : X^2_\alpha X^1_\alpha &amp; X^2_\alpha X^2_\alpha &amp; X^2_\alpha X^3_\alpha \\</a>
<a name="96"><span class="lineNum">      96 </span>            : X^1_\alpha X^3_\alpha &amp; X^3_\alpha X^2_\alpha &amp; X^3_\alpha X^3_\alpha</a>
<a name="97"><span class="lineNum">      97 </span>            : \end{array}\right] \nonumber \\</a>
<a name="98"><span class="lineNum">      98 </span>            : &amp;=&amp; \mathbf{c 1^T} + \mathbf{1 c^T} - 2 \sum_\alpha \mathbf{x}_a \mathbf{x}^T_a =  \mathbf{c 1^T} + \mathbf{1 c^T} - 2\mathbf{X X^T}</a>
<a name="99"><span class="lineNum">      99 </span>            : \f}</a>
<a name="100"><span class="lineNum">     100 </span>            : </a>
<a name="101"><span class="lineNum">     101 </span>            : This last equation can be extended to situations when we have more than three points.  In it \f$\mathbf{X}\f$ is a matrix that has</a>
<a name="102"><span class="lineNum">     102 </span>            : one high-dimensional point on each of its rows and \f$\mathbf{X^T}\f$ is its transpose.  \f$\mathbf{1}\f$ is an \f$M \times 1\f$ vector</a>
<a name="103"><span class="lineNum">     103 </span>            : of ones and \f$\mathbf{c}\f$ is a vector with components given by:</a>
<a name="104"><span class="lineNum">     104 </span>            : </a>
<a name="105"><span class="lineNum">     105 </span>            : \f[</a>
<a name="106"><span class="lineNum">     106 </span>            : c_i = \sum_\alpha (x_\alpha^i)^2</a>
<a name="107"><span class="lineNum">     107 </span>            : \f]</a>
<a name="108"><span class="lineNum">     108 </span>            : </a>
<a name="109"><span class="lineNum">     109 </span>            : These quantities are the diagonal elements of \f$\mathbf{X X^T}\f$, which is a dot product or Gram Matrix that contains the</a>
<a name="110"><span class="lineNum">     110 </span>            : dot product of the vector \f$X_i\f$ with the vector \f$X_j\f$ in element \f$i,j\f$.</a>
<a name="111"><span class="lineNum">     111 </span>            : </a>
<a name="112"><span class="lineNum">     112 </span>            : In classical scaling we introduce a centering matrix \f$\mathbf{J}\f$ that is given by:</a>
<a name="113"><span class="lineNum">     113 </span>            : </a>
<a name="114"><span class="lineNum">     114 </span>            : \f[</a>
<a name="115"><span class="lineNum">     115 </span>            : \mathbf{J} = \mathbf{I} - \frac{1}{M} \mathbf{11^T}</a>
<a name="116"><span class="lineNum">     116 </span>            : \f]</a>
<a name="117"><span class="lineNum">     117 </span>            : </a>
<a name="118"><span class="lineNum">     118 </span>            : where \f$\mathbf{I}\f$ is the identity.  Multiplying the equations above from the front and back by this matrix and a factor of a \f$-\frac{1}{2}\f$ gives:</a>
<a name="119"><span class="lineNum">     119 </span>            : </a>
<a name="120"><span class="lineNum">     120 </span>            : \f{eqnarray*}{</a>
<a name="121"><span class="lineNum">     121 </span>            :  -\frac{1}{2} \mathbf{J} \mathbf{D}^2(\mathbf{X}) \mathbf{J} &amp;=&amp; -\frac{1}{2}\mathbf{J}( \mathbf{c 1^T} + \mathbf{1 c^T} - 2\mathbf{X X^T})\mathbf{J} \\</a>
<a name="122"><span class="lineNum">     122 </span>            :  &amp;=&amp; -\frac{1}{2}\mathbf{J c 1^T J} - \frac{1}{2} \mathbf{J 1 c^T J} + \frac{1}{2} \mathbf{J}(2\mathbf{X X^T})\mathbf{J} \\</a>
<a name="123"><span class="lineNum">     123 </span>            :  &amp;=&amp; \mathbf{ J X X^T J } = \mathbf{X X^T } \label{eqn:scaling}</a>
<a name="124"><span class="lineNum">     124 </span>            : \f}</a>
<a name="125"><span class="lineNum">     125 </span>            : </a>
<a name="126"><span class="lineNum">     126 </span>            : The fist two terms in this expression disappear because \f$\mathbf{1^T J}=\mathbf{J 1} =\mathbf{0}\f$, where \f$\mathbf{0}\f$</a>
<a name="127"><span class="lineNum">     127 </span>            : is a matrix containing all zeros.  In the final step meanwhile we use the fact that the matrix of squared distances will not</a>
<a name="128"><span class="lineNum">     128 </span>            : change when we translate all the points.  We can thus assume that the mean value, \f$\mu\f$, for each of the components, \f$\alpha\f$:</a>
<a name="129"><span class="lineNum">     129 </span>            : \f[</a>
<a name="130"><span class="lineNum">     130 </span>            : \mu_\alpha = \frac{1}{M} \sum_{i=1}^N \mathbf{X}^i_\alpha</a>
<a name="131"><span class="lineNum">     131 </span>            : \f]</a>
<a name="132"><span class="lineNum">     132 </span>            : is equal to 0 so the columns of \f$\mathbf{X}\f$ add up to 0.  This in turn means that each of the columns of</a>
<a name="133"><span class="lineNum">     133 </span>            : \f$\mathbf{X X^T}\f$ adds up to zero, which is what allows us to write \f$\mathbf{ J X X^T J } = \mathbf{X X^T }\f$.</a>
<a name="134"><span class="lineNum">     134 </span>            : </a>
<a name="135"><span class="lineNum">     135 </span>            : The matrix of squared distances is symmetric and positive-definite we can thus use the spectral decomposition to decompose it as:</a>
<a name="136"><span class="lineNum">     136 </span>            : </a>
<a name="137"><span class="lineNum">     137 </span>            : \f[</a>
<a name="138"><span class="lineNum">     138 </span>            : \Phi= \mathbf{V} \Lambda \mathbf{V}^T</a>
<a name="139"><span class="lineNum">     139 </span>            : \f]</a>
<a name="140"><span class="lineNum">     140 </span>            : </a>
<a name="141"><span class="lineNum">     141 </span>            : Furthermore, because the matrix we are diagonalizing, \f$\mathbf{X X^T}\f$, is the product of a matrix and its transpose</a>
<a name="142"><span class="lineNum">     142 </span>            : we can use this decomposition to write:</a>
<a name="143"><span class="lineNum">     143 </span>            : </a>
<a name="144"><span class="lineNum">     144 </span>            : \f[</a>
<a name="145"><span class="lineNum">     145 </span>            : \mathbf{X} =\mathbf{V} \Lambda^\frac{1}{2}</a>
<a name="146"><span class="lineNum">     146 </span>            : \f]</a>
<a name="147"><span class="lineNum">     147 </span>            : </a>
<a name="148"><span class="lineNum">     148 </span>            : Much as in PCA there are generally a small number of large eigenvalues in \f$\Lambda\f$ and many small eigenvalues.</a>
<a name="149"><span class="lineNum">     149 </span>            : We can safely use only the large eigenvalues and their corresponding eigenvectors to express the relationship between</a>
<a name="150"><span class="lineNum">     150 </span>            : the coordinates \f$\mathbf{X}\f$.  This gives us our set of low-dimensional projections.</a>
<a name="151"><span class="lineNum">     151 </span>            : </a>
<a name="152"><span class="lineNum">     152 </span>            : This derivation makes a number of assumptions about the how the low dimensional points should best be arranged to minimize</a>
<a name="153"><span class="lineNum">     153 </span>            : the stress. If you use an interactive optimization algorithm such as SMACOF you may thus be able to find a better</a>
<a name="154"><span class="lineNum">     154 </span>            : (lower-stress) projection of the points.  For more details on the assumptions made</a>
<a name="155"><span class="lineNum">     155 </span>            : see &lt;a href=&quot;http://quest4rigor.com/tag/multidimensional-scaling/&quot;&gt; this website.&lt;/a&gt;</a>
<a name="156"><span class="lineNum">     156 </span>            : */</a>
<a name="157"><span class="lineNum">     157 </span>            : //+ENDPLUMEDOC</a>
<a name="158"><span class="lineNum">     158 </span>            : </a>
<a name="159"><span class="lineNum">     159 </span>            : namespace PLMD {</a>
<a name="160"><span class="lineNum">     160 </span>            : namespace dimred {</a>
<a name="161"><span class="lineNum">     161 </span>            : </a>
<a name="162"><span class="lineNum">     162 </span><span class="lineCov">         14 : class ClassicalMultiDimensionalScaling : public DimensionalityReductionBase {</span></a>
<a name="163"><span class="lineNum">     163 </span>            : public:</a>
<a name="164"><span class="lineNum">     164 </span>            :   static void registerKeywords( Keywords&amp; keys );</a>
<a name="165"><span class="lineNum">     165 </span>            :   explicit ClassicalMultiDimensionalScaling( const ActionOptions&amp; ao );</a>
<a name="166"><span class="lineNum">     166 </span>            :   void calculateProjections( const Matrix&lt;double&gt;&amp;, Matrix&lt;double&gt;&amp; ) override;</a>
<a name="167"><span class="lineNum">     167 </span>            : };</a>
<a name="168"><span class="lineNum">     168 </span>            : </a>
<a name="169"><span class="lineNum">     169 </span><span class="lineCov">       7846 : PLUMED_REGISTER_ACTION(ClassicalMultiDimensionalScaling,&quot;CLASSICAL_MDS&quot;)</span></a>
<a name="170"><span class="lineNum">     170 </span>            : </a>
<a name="171"><span class="lineNum">     171 </span><span class="lineCov">          8 : void ClassicalMultiDimensionalScaling::registerKeywords( Keywords&amp; keys ) {</span></a>
<a name="172"><span class="lineNum">     172 </span><span class="lineCov">          8 :   DimensionalityReductionBase::registerKeywords( keys );</span></a>
<a name="173"><span class="lineNum">     173 </span><span class="lineCov">          8 : }</span></a>
<a name="174"><span class="lineNum">     174 </span>            : </a>
<a name="175"><span class="lineNum">     175 </span><span class="lineCov">          7 : ClassicalMultiDimensionalScaling::ClassicalMultiDimensionalScaling( const ActionOptions&amp; ao):</span></a>
<a name="176"><span class="lineNum">     176 </span>            :   Action(ao),</a>
<a name="177"><span class="lineNum">     177 </span><span class="lineCov">          7 :   DimensionalityReductionBase(ao)</span></a>
<a name="178"><span class="lineNum">     178 </span>            : {</a>
<a name="179"><span class="lineNum">     179 </span><span class="lineCov">          7 :   if( dimredbase ) error(&quot;input to CLASSICAL_MDS should not be output from dimensionality reduction object&quot;);</span></a>
<a name="180"><span class="lineNum">     180 </span><span class="lineCov">          7 : }</span></a>
<a name="181"><span class="lineNum">     181 </span>            : </a>
<a name="182"><span class="lineNum">     182 </span><span class="lineCov">         12 : void ClassicalMultiDimensionalScaling::calculateProjections( const Matrix&lt;double&gt;&amp; targets, Matrix&lt;double&gt;&amp; projections ) {</span></a>
<a name="183"><span class="lineNum">     183 </span>            :   // Retrieve the distances from the dimensionality reduction object</a>
<a name="184"><span class="lineNum">     184 </span><span class="lineCov">         12 :   double half=(-0.5); Matrix&lt;double&gt; distances( half*targets );</span></a>
<a name="185"><span class="lineNum">     185 </span>            : </a>
<a name="186"><span class="lineNum">     186 </span>            :   // Apply centering transtion</a>
<a name="187"><span class="lineNum">     187 </span>            :   unsigned n=distances.nrows(); double sum;</a>
<a name="188"><span class="lineNum">     188 </span>            :   // First HM</a>
<a name="189"><span class="lineNum">     189 </span><span class="lineCov">       1564 :   for(unsigned i=0; i&lt;n; ++i) {</span></a>
<a name="190"><span class="lineNum">     190 </span><span class="lineCov">     387504 :     sum=0; for(unsigned j=0; j&lt;n; ++j) sum+=distances(i,j);</span></a>
<a name="191"><span class="lineNum">     191 </span><span class="lineCov">     387504 :     for(unsigned j=0; j&lt;n; ++j) distances(i,j) -= sum/n;</span></a>
<a name="192"><span class="lineNum">     192 </span>            :   }</a>
<a name="193"><span class="lineNum">     193 </span>            :   // Now (HM)H</a>
<a name="194"><span class="lineNum">     194 </span><span class="lineCov">       1552 :   for(unsigned i=0; i&lt;n; ++i) {</span></a>
<a name="195"><span class="lineNum">     195 </span><span class="lineCov">     387504 :     sum=0; for(unsigned j=0; j&lt;n; ++j) sum+=distances(j,i);</span></a>
<a name="196"><span class="lineNum">     196 </span><span class="lineCov">     387504 :     for(unsigned j=0; j&lt;n; ++j) distances(j,i) -= sum/n;</span></a>
<a name="197"><span class="lineNum">     197 </span>            :   }</a>
<a name="198"><span class="lineNum">     198 </span>            : </a>
<a name="199"><span class="lineNum">     199 </span>            :   // Diagonalize matrix</a>
<a name="200"><span class="lineNum">     200 </span><span class="lineCov">         12 :   std::vector&lt;double&gt; eigval(n); Matrix&lt;double&gt; eigvec(n,n);</span></a>
<a name="201"><span class="lineNum">     201 </span><span class="lineCov">         12 :   diagMat( distances, eigval, eigvec );</span></a>
<a name="202"><span class="lineNum">     202 </span>            : </a>
<a name="203"><span class="lineNum">     203 </span>            :   // Pass final projections to map object</a>
<a name="204"><span class="lineNum">     204 </span><span class="lineCov">       1552 :   for(unsigned i=0; i&lt;n; ++i) {</span></a>
<a name="205"><span class="lineNum">     205 </span><span class="lineCov">      13960 :     for(unsigned j=0; j&lt;projections.ncols(); ++j) projections(i,j)=sqrt(eigval[n-1-j])*eigvec(n-1-j,i);</span></a>
<a name="206"><span class="lineNum">     206 </span>            :   }</a>
<a name="207"><span class="lineNum">     207 </span><span class="lineCov">         12 : }</span></a>
<a name="208"><span class="lineNum">     208 </span>            : </a>
<a name="209"><span class="lineNum">     209 </span>            : }</a>
<a name="210"><span class="lineNum">     210 </span><span class="lineCov">       5874 : }</span></a>
</pre>
      </td>
    </tr>
  </table>
  <br>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="ruler"><img src="../glass.png" width=3 height=3 alt=""></td></tr>
    <tr><td class="versionInfo">Generated by: <a href="http://ltp.sourceforge.net/coverage/lcov.php" target="_parent">LCOV version 1.14</a></td></tr>
  </table>
  <br>

</body>
</html>
